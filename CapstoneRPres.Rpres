Elementary Text Prediction
========================================================
A JHU Data Science Specialization Capstone Project


Wat Hughes

wat.hughes@gmail.com

January 15, 2016

The Predicition Algorithm
========================================================

This application uses a model known as Stupid Backoff. That is introduced in section 4 of Brants et. al.'s paper http://www.aclweb.org/anthology/D07-1090.pdf where it is noted that, despite the name, the model performs well if given enough appropriate training data.

Stupid Backoff is a specific language model of the ngram family. The model is trained by constructing ngrams (1-4 in this case) from the corpus. Predictions are easily computed using maximum-likelihood estimation when a matching ngram exists. Stupid Backoff allows the model to handle words and phrases that were not in the training corpus. Per the referenced paper, there is a backoff parameter, called alpha (but called lambda in other sources) that is not needed by this application since we are always predicting 1 word and not ranking a series of predictions.

The Application
========================================================

This application helps authors who are typing a phrase and find themselves stuck for an idea for the next word. There are two modes. Each mode provides help for a distinct writing style. The first mode helps users of Twitter compose their Tweets. The second mode helps with longer and more formal pieces of writing. Each mode is available on its own tab. The user can go back and forth as needed.

The application also keeps track of everything the author writes, each predicted next word, and each actual word chosen by the user. That information is on the last tab.

To get started, pick a mode from the tab bar. Starting writing a phrase in the indicated box. When stuck for a word, use the Predict button to get a suggestion. You can then either accept that or enter a better word. When happy with the word, press the Accept button. When you no longer need a suggestion for your next word, press the Close button.

Try It
========================================================

This application is deployed to the web curtesdy of R Studio and ShinyApps.io. Try it here: https://wathughes.shinyapps.io/Capstone/ and let me know what you think.

Note that the size of the ngram models were severely limited to accomadate the RAM limits of the free hosting at ShinyApps.io.

This application was developed as part of the Johns Hopkins University Data Science Specialiation. For more info: https://www.coursera.org/specializations/jhu-data-science/.

Thanks to Hans Christensen of HC Corpora for the data (via Coursera:
https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip) used to train the models that drive the predictions. For more information about this data: http://www.corpora.heliohost.org/.

Next Steps
========================================================
If time had permitted, these would have been my next steps:

- Implment more writing style modes by training against appropriate style-specific corpus sources
- Improve the models:
- LSTM models have been shown to perform better for this purpose than the implemented ngram model
- More info on LSTM models: http://colah.github.io/posts/2015-08-Understanding-LSTMs/
- Implement part of speech tagging
- More info: http://www.martinschweinberger.de/blog/part-of-speech-tagging-with-r/
- Focus group test the modes to pick one to convert from Shiny to a tablet/mobile application
