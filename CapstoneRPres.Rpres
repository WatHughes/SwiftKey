PhraseMaster
========================================================
A JHU Data Science Specialization Capstone Project
Featuring Elementary Text Prediction

Wat Hughes

wat.hughes@gmail.com

January 23, 2016

The Prediction Algorithm
========================================================

This application uses a model known as Stupid Backoff. That model is introduced in section 4 of Brants et al.'s paper http://www.aclweb.org/anthology/D07-1090.pdf where it is noted that, despite the name, the model performs well if given enough appropriate training data.

Stupid Backoff is a specific language model of the ngram family. The model is trained by constructing ngrams (n = 1-4 here) from the corpus. Predictions are easily computed using maximum-likelihood estimation when a matching ngram exists. Stupid Backoff allows the model to handle words and phrases not in the training corpus. Brants et al. described a backoff parameter, alpha (lambda in other sources). That parameter is not needed here since we are always predicting 1 word and not ranking a series of predictions.

The Application
========================================================

The PhraseMaster application helps authors who are typing a phrase and need an idea for the next word. There are two modes for distinct writing styles. The first mode helps Twitter users compose their Tweets. The second mode helps with longer and more formal pieces of writing. Each mode is available on its own tab. The user can go back and forth as needed.

The application keeps track of everything the author writes, each predicted next word, and each word chosen by the user. That information is on the last tab.

To get started, pick a mode from the tab bar and  start writing a phrase in the indicated box. Use the Predict button to get a suggestion for the next word in your phrase. You can then either accept that or enter a better word. When satisfied, press the Accept button. When finished, press the Close button.


Try It
========================================================

This application is deployed to the web courtesy of R Studio and ShinyApps.io. Try it here: https://wathughes.shinyapps.io/Capstone/.

Note that the sizes of the ngram models were severely limited to accommodate the RAM limits of the free hosting at ShinyApps.io and to permit speedy response to the user.

This application was developed as part of the Johns Hopkins University Data Science Specialization For more info: https://www.coursera.org/specializations/jhu-data-science/.

Thanks to Hans Christensen of HC Corpora for the data: https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip This was used to train the models that drive the predictions. For more information about this data: http://www.corpora.heliohost.org/.

Next Steps
========================================================
If time had permitted, these would have been my next steps:

- Implement more writing style modes by training against appropriate style-specific corpus sources
- Improve the models:
    - LSTM models have been shown to perform better for this purpose than the implemented ngram model
    - More info on LSTM models: http://colah.github.io/posts/2015-08-Understanding-LSTMs/
- Implement part of speech tagging
- More info: http://www.martinschweinberger.de/blog/part-of-speech-tagging-with-r/
- Focus group test the modes to pick one to convert from Shiny to a tablet/mobile application
